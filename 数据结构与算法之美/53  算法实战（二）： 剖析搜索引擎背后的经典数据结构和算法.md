[TOC]

## 53 | 算法实战（二）： 剖析搜索引擎背后的经典数据结构和算法

- 今天，我就借助搜索引擎，这样一个非常有技术含量的产品，来给你展示一下，数据结构和算法是如何应用在其中的。

### 整体系统介绍

- 搜索引擎大致可以分为四个部分：
    - **搜集**
        - 利用爬虫爬取网页
    - **分析**
        - 负责网页内容抽取、分词，构建临时索引，计算 PageRank 值这几部分工作
    - **索引**
        - 通过临时索引，构建倒排索引
    - **查询**
        - 响应用户请求，根据倒排索引获取相关网页，计算网页排名，返回查询结果。

### 搜集

- 搜索引擎把整个互联网看作数据结构中的有向图，把每个页面看作一个顶点。如果某个页面上包含另一个页面的链接，那我们就在两个顶点间连一条有向边。我们可以利用图的遍历搜索算法，来遍历整个互联网中的网页。
- 搜索引擎采用的是**广度优先搜索策略**。（原理）
    - 具体点讲的话，那就是，我们先找一些比较知名的网页的链接，作为种子网页链接，放入队列中。爬虫按广度优先策略，不停的从队列中取出链接，然后取爬取对应的网页，解析网页里包含的其他网页链接，再将解析出来的链接添加到队列中。

#### 1. 待抓取网页链接文件：links.bin

- 在爬取网页的过程中，爬虫会不停的解析页面链接，将其放到队列中。于是队列中的链接就会越来越多，可能会多到内存放不下。
- 所以，**我们用一个存储在磁盘中的文件（links.bin）来作为队列**。爬虫将网页解析出来的链接直接存储到 links.bin 文件中。
- 这样用文件的方法来存储网页链接的方式，还有其他好处。如，支持断点续爬。也就说，当机器断电后，网页链接不会丢失。当机器重启后，还可以从之前爬取的位置继续爬取。

#### 2. 网页判重文件： bloom_filter.bin

- 使用**布隆过滤器**，我们就可以快速并且非常节省内存地实现网页的判重。
- 不过，还是刚刚那个问题，如果我们把布隆过滤器存储在内存中，一旦机器宕机重启后，布隆过滤器就被清空了。这样就可能导致大量已经爬取的网页会被重复爬取。
- 如果解决这个问题呢？
    - 我们可以定期的**将布隆过滤器持久化到磁盘中，存储在 bloomfilter.bin 文件中**。
    - 这样，即使出现机器宕机，也只会丢布隆过滤器中部分数据。
    - 当机器重启后，我们就可以重新读取磁盘中的 bloomfilter.bin 文件，将其恢复到内存中。

#### 3. 原始网页存储文件：doc_raw.bin

- 爬取到网页后，我们需要将其存储下来，心备后面离线分析、索引用之用。
- 如果我们把每个网页都存储为一个独立的文本，那磁盘中的文件就会非常多，数量可能会有几千万，甚至上亿。
- **我们可以把多个网页存储在一个文件中。每个网页通过一定的标识进行分隔，方便后续读取。**如下图：
    - ![img](https://static001.geekbang.org/resource/image/19/4d/195c9a1dceaaa9f4d2483fa91455404d.jpg)
- 当然，**我们也要对文件大小做一定的限制**。如，每个文件不能超过 1GB，当超过的时候，我们就创建一个新文件。

#### 4. 网页链接及其编号的对应文件：doc_id.bin

- 网页编号实际上就是给每个网页分配的一个唯一的 ID，方便我们后续对网页进行分析、索引。
- 在存储网页的同时，我们将网页链接跟编号之间的对应关系，存储在另一个 doc_id.bin 文件中。

#### 5. 小结

- 爬虫在爬取网页的过程中，涉及的四个重要的文件。
    - **links.bin和 bloom_filter.bin ， 这两个文件是爬虫自身所用的。**
    - **doc_raw.bin 和 doc_id.bin 是作为搜集阶段的成果，供后面分析、索引、查询用的。**

### 分析

- 网页爬取下来后，我们需要对网页进行离线分析。

#### 1. 抽取网页文本信息

- 依靠 HTML 标签来抽取网页中的文件信息。
    1. 去掉 JavaScript 代码、CSS 格式
    2. 去掉所有的 HTML 标签

#### 2. 分词并创建临时索引

- 分词
    - 英文网页
        - 通过空格、标点符号等分隔符，将每个单词分割开就可以了。
    - 中文网页
        - 基于字典和规则的分词法
            - 采用**最长匹配规则**。匹配尽可能长的词语。
            - 实现
                - 将词库中的单词，构建成 Trie 树结构，然后拿网页文本在 Trie 树中匹配。
- 临时索引
    - 分词完成后，我们得到一组单词列表。**我们把单词与网页之间的对应关系，写入到一个临时索引文件中（tmp_index.bin）**。这个临时索引文件用来构建倒排索引文件。
    - 临时索引文件格式如下：
        - ![img](https://static001.geekbang.org/resource/image/15/1e/156ee98c0ad5763a082c1f3002d6051e.jpg)
        - 图中的 iterm_id，存储的是单词编号，而非单词本身。这样的目的主要是为了节省存储空间。
- 单词编号文件
    - 给单词编号的方式，跟网页编号类似。
    - 这个过程中，我们还需要使用散列表，记录已经编过号的单词。
        - 我们拿到分割出来的单词，先到散列表中查找
            - 如果找到，那就直接使用已有的编号。
            - 如果没找到，我们再去计数器中拿号码，并且将这个新单词及编号添加到散列表中。
    - 当所有的网页处理完成后，我们再将**单词跟编号之间的对应关系，写入到磁盘文件中，并命名为 iterm_id.bin。**

#### 3. 小结

-  分析阶段，我们得到了两个重要的文件
    - **临时索引文件 tmp_index.bin**
    - **单词编号文件 term_id.bin**

### 索引

- 将临时索引构建成**倒排索引**。
- 倒排索引中记录了每个单词以及包含它的网页列表。如下图：
    - ![img](https://static001.geekbang.org/resource/image/de/34/de1f212bc669312a499bbbf2ee3a3734.jpg)
- 如何通过临时索引文件，构建出倒排索引文件呢？
    - 临时索引文件很大，无法一次加载到内存中，搜索引擎一般会选择使用**多路归并排序**的方法来实现。
    - 这里将其分割成多个小文件，先对每个文件独立排序，最后合并在一起。在实际开发中，我们其实可以直接利用 MapReduce 来处理。
    - 排序完成后，相同的单词就被排列到了一起。我们只需要顺序地遍历排好序的临时索引文件，就能将每个单词对应的网页编号列表找出来，然后，把它们存储在倒排索引文件中。具体处理过程，如下图：
        - ![img](https://static001.geekbang.org/resource/image/c9/e6/c91c960472d88233f60d5d4ce6538ee6.jpg)
- 除了倒排索引文件外，我们还需要一个文件，**来记录每个单词编号在倒排索引文件中的偏移位置**。我们把这个文件命名为 **term_offset.bin**。
    - 这个文件的作用是，帮助我们快速地查找到某个单词编号在倒排索引中存储的位置，进而快速地从倒排索引中读取单词编号对应的网页编号列表。
    - ![img](https://static001.geekbang.org/resource/image/de/54/deb2fd01ea6f7e1df9da1ad3a8da5854.jpg)
- 小结
    - 经过索引阶段的处理，我们得到了两个有价值的文件：
        - **倒排索引文件 index.bin**
        - **记录单词编号在索引文件中的偏移位置的文件 term_offset.bin** 

### 查询

- 前几步产生的几个文件：
    - doc_id.bin 记录网页链接和编号之间的对应关系
    - term_id.bin 记录单词和编号之间的对应关系
    - index.bin 倒排索引文件，记录每个单词编号以及对应包含它的网页编号列表
    - term_offset.bin 记录每个单词编号在倒排索引文件中的偏移位置
- 这几个文件中，除了倒排索引（index.bin）外，其他都比较小。为了方便查找数据，我们将其他三个文件都加载到内存中，并组织成散列表这种数据结构。
- 查询过程
    1. 用户在搜索框，输入某个查询文本的时候，我们先用用户输入文本进行分词处理。假设，分词后，我们得到 k 个单词。
    2. 我们拿这 k 个单词，去 term_id.bin 对应的散列表中，查找对应的单词编号。之后，我们拿到了这个 k 个单词对应的单词编号。
    3. 我们拿这 k 个单词编号，去term_offset.bin 对应的散列表中，查找每个单词编号在倒排索引文件中的偏移位置。之后，我们得到了 k 个偏移位置。
    4. 我们拿这 k 个偏移位置，去倒排索引（index.bin）中，查找 k 单词对应的包含它的网页编号列表。之后，我们得到了 k 个网页编号列表。
    5. 我们针对这 k 个网页编号列表，统计每个网页编号出现的次数。然后，按出现的次数，从小到大排序。出现次数越多，说明包含越多的用户查询单词。
    6. 最后，我们得到一组排好序的网页编号。我们拿着编号，去 doc_id.bin 文件中查找对应的网页链接，分页显示给用户就可以了。

### 总结引申

- 今天 ，我给你展示了一个小型搜索引擎的设计思路。这时里只是一个搜索引擎的设计基本原理。
- 我们涉及的数据结构和算法有：
    - 图
    - 散列表
    - Trie 树
    - 布隆过滤器
    - 单模式字符串匹配算法
    - AC 自动机
    - 广度优先遍历
    - 归并排序
- 这里强烈建议你，按这个思路，实现一个简单的搜索引擎。

### 课后思考

1. 图在遍历方法在两种：深度优先、广度优先。爬虫为什么选择广度优先策略，而不是深度优先策略呢？
2. 大部分搜索引擎的结果显示的时候，都支持摘要信息和网页快照。这是如何实现的呢？

#### 精选一

1. 因为搜索引擎要优先爬取权重较高的页面，离种子网页越近，较大可能权重更高，广度优先更合适。